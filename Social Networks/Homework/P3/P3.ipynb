{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected degree and expected average degree of neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a deterministic graph $G(V,E)$ and the random variable $X$ being the grade of a random node in $V$. Let's define also the random variable $Y$ as the average degree of the neighborhood of a random node. \n",
    "\n",
    "If we define a uniform sampling scheme over the nodes, we have:\n",
    "\n",
    "$$ \\mathbb{E}[X] = \\frac{1}{|V|} \\sum_{v \\in V} d_v $$\n",
    "$$ \\mathbb{E}[Y] = \\frac{1}{|V|} \\sum_{v \\in V} \\frac{1}{|V(v)|} \\sum_{u \\in V(v)} d_u $$\n",
    "\n",
    "where $V(v)$ is the neighborhood of node $v$, so that clearly it is $|V(v)| = d_v$: \n",
    "\n",
    "$$ \\mathbb{E}[X] = \\frac{1}{|V|} \\sum_{v \\in V} d_v $$\n",
    "$$ \\mathbb{E}[Y] = \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V(v)} \\frac{d_u}{d_v} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It clearly follows that for $d$-regular graph, where $d_w = d$ for each $w \\in V$, $\\mathbb{E}[X] = \\mathbb{E}[Y]$: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\mathbb{E}[X] = \\frac{1}{|V|} \\sum_{v \\in V} d = \\frac{1}{|V|} |V|d = d $$\n",
    "$$ \\mathbb{E}[Y] = \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V(v)} \\frac{d}{d} =  \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V(v)} 1 = \\frac{1}{|V|} \\sum_{v \\in V} d = \\frac{1}{|V|} |V|d = d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to show that instead, without regularity assumption, $\\mathbb{E}[X] \\leq \\mathbb{E}[Y]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's highlight that:\n",
    "$$ \\mathbb{E}[Y] = \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V(v)} \\frac{d_u}{d_v} = \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V} A_{uv}\\frac{d_u}{d_v}$$\n",
    "\n",
    "where is the adjacency matrix of the graph, that works in this case like an indicator function for the inner sum over the neighborhood of the node $v$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the quantity $\\mathbb{E}[X] - \\mathbb{E}[Y]$:\n",
    "\n",
    "$$ \\mathbb{E}[X] - \\mathbb{E}[Y] =\\frac{1}{|V|} \\sum_{v \\in V} d_v - \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V} A_{uv}\\frac{d_u}{d_v} =\n",
    "\\frac{1}{|V|} \\sum_{v \\in V}(d_v - \\sum_{u \\in V} A_{uv}\\frac{d_u}{d_v} ) = \\frac{1}{|V|} \\sum_{v \\in V} \\sum_{u \\in V} A_{uv}( 1 - \\frac{d_u}{d_v} )$$\n",
    "\n",
    "where the last step comes from the fact that the degree of a node is the sum over its respective column (or row) in the adjacency matrix.\n",
    "So we have:\n",
    "\n",
    "$$ \\mathbb{E}[X] - \\mathbb{E}[Y] =\\frac{1}{|V|} \\sum_{v,u} A_{uv}( 1 - \\frac{d_u}{d_v} )$$\n",
    "\n",
    "Being the adjacency matrix symmetric, this double sum is perfectly equivalent to:\n",
    "\n",
    "$$ \\mathbb{E}[X] - \\mathbb{E}[Y] =\\frac{1}{|V|} \\sum_{u,v} A_{uv}( 1 - \\frac{d_v}{d_u} )$$\n",
    "\n",
    "So we can sum these two expressions:\n",
    "\n",
    "$$ 2(\\mathbb{E}[X] - \\mathbb{E}[Y]) =\\frac{1}{|V|} \\sum_{u,v} A_{uv}[2 - (\\frac{d_u}{d_v} + \\frac{d_v}{d_u})]$$\n",
    "$$ \\mathbb{E}[X] - \\mathbb{E}[Y] =\\frac{1}{2|V|} \\sum_{u,v} A_{uv}[2 - (\\frac{d_u}{d_v} + \\frac{d_v}{d_u})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can leverage the following: \n",
    "\n",
    "$$ \\frac{a}{b} + \\frac{b}{a} \\geq 2 $$\n",
    "$$ \\frac{a^2 + b^2}{ab} \\geq 2 \\rightarrow a^2 + b^2 \\geq 2ab \\rightarrow a^2 + b^2 - 2ab \\geq 0 \\rightarrow (a-b)^2 \\geq 0 $$\n",
    "\n",
    "that is clearly true $\\forall a,b \\in \\mathbb{R}$.\n",
    "\n",
    "Since we have that $(\\frac{d_u}{d_v} + \\frac{d_v}{d_u}) \\geq 2$, all the terms inside the sum $2 - (\\frac{d_u}{d_v} + \\frac{d_v}{d_u})$ are nonpositive. \n",
    "\n",
    "This implies that over all $\\mathbb{E}[X] - \\mathbb{E}[Y] \\leq 0$, that means $\\mathbb{E}[X] \\leq \\mathbb{E}[Y]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
